{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:26:44.262468Z","iopub.execute_input":"2021-12-14T08:26:44.262992Z","iopub.status.idle":"2021-12-14T08:26:51.377020Z","shell.execute_reply.started":"2021-12-14T08:26:44.262873Z","shell.execute_reply":"2021-12-14T08:26:51.374547Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 1168453109350621018\n, name: \"/device:GPU:0\"\ndevice_type: \"GPU\"\nmemory_limit: 16152002560\nlocality {\n  bus_id: 1\n  links {\n  }\n}\nincarnation: 15519934931380951503\nphysical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n]\n","output_type":"stream"},{"name":"stderr","text":"2021-12-14 08:26:49.201043: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2021-12-14 08:26:49.264154: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:49.367800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:49.369008: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:51.360963: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:51.362262: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:51.363410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:51.364388: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\ntf.test.is_gpu_available()","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:26:54.727559Z","iopub.execute_input":"2021-12-14T08:26:54.727919Z","iopub.status.idle":"2021-12-14T08:26:54.748928Z","shell.execute_reply.started":"2021-12-14T08:26:54.727886Z","shell.execute_reply":"2021-12-14T08:26:54.747921Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2021-12-14 08:26:54.730511: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:54.731624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:54.732629: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:54.733786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:54.734917: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2021-12-14 08:26:54.735943: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 15403 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from numpy import array\nfrom numpy import argmax\nfrom pandas import DataFrame\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model\nfrom keras.models import Model\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\nfrom keras.layers import Embedding\nfrom keras.layers.merge import add\nfrom keras.layers import Dropout\nfrom numpy import argmax\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:26:57.957163Z","iopub.execute_input":"2021-12-14T08:26:57.957494Z","iopub.status.idle":"2021-12-14T08:27:00.236311Z","shell.execute_reply.started":"2021-12-14T08:26:57.957460Z","shell.execute_reply":"2021-12-14T08:27:00.235237Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def LoadDoc(file_name):\n    file = open(file_name, 'r', encoding = \"utf-8\")\n    text_data = file.read()\n    file.close()\n    return text_data\n\n\n# In[14]:\n\n\ndef LoadPhotoFeatures(file_name, dataset):\n    AllFeatures = load(open(file_name, 'rb'))\n    features = {k[:-4]: AllFeatures[k[:-4]] for k in dataset}\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:27:00.243054Z","iopub.execute_input":"2021-12-14T08:27:00.245782Z","iopub.status.idle":"2021-12-14T08:27:00.259990Z","shell.execute_reply.started":"2021-12-14T08:27:00.245720Z","shell.execute_reply":"2021-12-14T08:27:00.258013Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def LoadPhotoFeatures_2(file_name, dataset):\n    AllFeatures = load(open(file_name, 'rb'))\n    #print(AllFeatures.keys())\n    features = {}\n    for k in dataset:\n        if k.split(\".\")[0] in AllFeatures.keys():\n            features[k.split(\".\")[0]] = AllFeatures[k.split(\".\")[0]]\n    #features = { k[:-4]: AllFeatures[k[:-4]] for k in dataset}\n    return features","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:27:00.897855Z","iopub.execute_input":"2021-12-14T08:27:00.898190Z","iopub.status.idle":"2021-12-14T08:27:00.905465Z","shell.execute_reply.started":"2021-12-14T08:27:00.898158Z","shell.execute_reply":"2021-12-14T08:27:00.904136Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def LoadCleanDescription(file_name):\n    document = LoadDoc(file_name)\n    descriptions = dict()\n    i = 0\n    for l in document.split('\\n'):\n        if len(l)<1:\n            print(l,\":\",i)\n            break\n        i +=1\n        if i == 1:\n            continue\n        tokens = l.split(',')\n        imageId, imageDesc = tokens[0], tokens[1].split()\n        if imageId not in descriptions:\n            descriptions[imageId] = list()\n        desc = 'startseq ' + ' '.join(imageDesc) + ' endseq'\n        descriptions[imageId].append(desc)\n    return descriptions","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:27:03.888312Z","iopub.execute_input":"2021-12-14T08:27:03.888919Z","iopub.status.idle":"2021-12-14T08:27:03.900507Z","shell.execute_reply.started":"2021-12-14T08:27:03.888867Z","shell.execute_reply":"2021-12-14T08:27:03.897401Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def ToLines(desc):\n    all_desc = list()\n    for key in desc.keys():\n        [all_desc.append(d) for d in desc[key]]\n    return all_desc\n\n\n# In[17]:\n\n\ndef CreateTokenizer(desc):\n    l = ToLines(desc)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(l)\n    return tokenizer\n\n\n# In[18]:\n\n\ndef maxLength(descs):\n    lines = ToLines(descs)\n    return max(len(d.split()) for d in lines)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:27:04.474561Z","iopub.execute_input":"2021-12-14T08:27:04.474907Z","iopub.status.idle":"2021-12-14T08:27:04.483303Z","shell.execute_reply.started":"2021-12-14T08:27:04.474875Z","shell.execute_reply":"2021-12-14T08:27:04.482166Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def DefineModel(vocabSize, maxLength):\n    inputs1 = Input(shape=(4096,))\n    fe1 = Dropout(0.5, seed = 121)(inputs1)\n    fe2 = Dense(128, activation='relu')(fe1)\n    fe3 = RepeatVector(max_length)(fe2)\n    \n    # embedding\n    inputs2 = Input(shape=(max_length,))\n    emb2 = Embedding(vocab_size, 50, mask_zero=True)(inputs2)\n    emb2a = Dropout(0.5, seed = 121)(emb2)\n    emb3 = LSTM(128, return_sequences=True)(emb2a)\n    emb4 = LSTM(128, return_sequences=True)(emb3)\n    emb5 = TimeDistributed(Dense(128, activation='relu'))(emb4)\n    # merge inputs\n    merged = add([fe3, emb5])\n    # language model (decoder)\n    lm1 = Dropout(0.5, seed = 121)(merged)\n    lm2 = LSTM(500)(lm1)\n    lm3 = Dense(500, activation='relu')(lm2)\n    outputs = Dense(vocab_size, activation='softmax')(lm3)\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer = 'adam' , metrics=['accuracy'])\n    print(model.summary())\n    plot_model(model, show_shapes=True, to_file='plot.png')\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:27:05.605461Z","iopub.execute_input":"2021-12-14T08:27:05.606036Z","iopub.status.idle":"2021-12-14T08:27:05.619421Z","shell.execute_reply.started":"2021-12-14T08:27:05.606004Z","shell.execute_reply":"2021-12-14T08:27:05.618189Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def CreateSequence(tokenizer, maxLength, desc, images, vocabSize):\n    X1, X2, y = list(), list(), list()\n    for key, desc_list in desc.items():\n        #print(key)\n        for desc_sent in desc_list:\n            #print(desc_sent)\n            sequence = tokenizer.texts_to_sequences([desc_sent])[0]\n            for i in range(1, len(sequence)):\n                in_sequence, out_sequence = sequence[:i], sequence[i]\n                in_sequence = pad_sequences([in_sequence], maxlen=maxLength)[0]\n                out_sequence = to_categorical([out_sequence], num_classes=vocabSize)[0]\n                X1.append(images[key.split('.')[0]][0])\n                X2.append(in_sequence)\n                y.append(out_sequence)\n                # break\n            #break\n        #break\n    return array(X1), array(X2), array(y)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:27:06.438336Z","iopub.execute_input":"2021-12-14T08:27:06.438602Z","iopub.status.idle":"2021-12-14T08:27:06.449043Z","shell.execute_reply.started":"2021-12-14T08:27:06.438572Z","shell.execute_reply":"2021-12-14T08:27:06.446711Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#! cat ../input/setfiles/valid_file.tsv ../input/setfiles/train_file.tsv  > combined_train_val.csv\n! cat ../input/ic-train/ic_train.csv ../input/indiancaptionspart2/ic_captions_processed_part_2.csv > combined_indian.csv","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:28:16.938195Z","iopub.execute_input":"2021-12-14T08:28:16.938563Z","iopub.status.idle":"2021-12-14T08:28:17.832520Z","shell.execute_reply.started":"2021-12-14T08:28:16.938521Z","shell.execute_reply":"2021-12-14T08:28:17.831370Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_descriptions_set_2 = LoadCleanDescription('./combined_indian.csv')\ntrain_descriptions_set_1 = LoadCleanDescription('../input/combined-train-val/combined_train_val.csv')\n\ntrain_descriptions = {**train_descriptions_set_1 ,** train_descriptions_set_2}\n# test_descriptions = LoadCleanDescription('/kaggle/input/setfiles/test_file.tsv')\n#valid_descriptions = LoadCleanDescription('../valid_file.tsv')\ntrain_descriptions.pop('image_file')\nprint('Descriptions:',len(train_descriptions))","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:28:35.170776Z","iopub.execute_input":"2021-12-14T08:28:35.171117Z","iopub.status.idle":"2021-12-14T08:28:35.385778Z","shell.execute_reply.started":"2021-12-14T08:28:35.171082Z","shell.execute_reply":"2021-12-14T08:28:35.384646Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":" : 177\n : 32365\nDescriptions: 6649\n","output_type":"stream"}]},{"cell_type":"code","source":"# k1 = list(train_descriptions.keys())\n# k2 = list(train_features.keys())\n# for k in k1:\n#     if k.split('.')[0] not in k2:\n#         print(k)\n        ","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-12-14T05:26:55.377795Z","iopub.execute_input":"2021-12-14T05:26:55.378341Z","iopub.status.idle":"2021-12-14T05:26:55.382427Z","shell.execute_reply.started":"2021-12-14T05:26:55.378298Z","shell.execute_reply":"2021-12-14T05:26:55.381552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features_set_1 = LoadPhotoFeatures_2('../input/d/sayalighodekar/vgg16features/VGG16features.pkl', train_descriptions_set_1)\ntrain_features_set_2 = LoadPhotoFeatures_2('../input/india-vgg16/india_VGG16features.pkl', train_descriptions_set_2)\ntrain_features = {**train_features_set_1 ,** train_features_set_2}\nprint(\"len of train feats\", len(train_features))","metadata":{"execution":{"iopub.status.busy":"2021-12-14T08:28:54.019542Z","iopub.execute_input":"2021-12-14T08:28:54.019857Z","iopub.status.idle":"2021-12-14T08:28:55.302812Z","shell.execute_reply.started":"2021-12-14T08:28:54.019823Z","shell.execute_reply":"2021-12-14T08:28:55.300403Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"len of train feats 6562\n","output_type":"stream"}]},{"cell_type":"code","source":"#print('Photos: train=%d, valid=%d' % (len(train_features), len(valid_features)))\n\ntokenizer = CreateTokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = maxLength(train_descriptions)\nprint('Description Length: %d' % max_length)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T05:26:57.748785Z","iopub.execute_input":"2021-12-14T05:26:57.749258Z","iopub.status.idle":"2021-12-14T05:26:58.529851Z","shell.execute_reply.started":"2021-12-14T05:26:57.749218Z","shell.execute_reply":"2021-12-14T05:26:58.529129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport keras","metadata":{"execution":{"iopub.status.busy":"2021-12-14T05:27:00.343726Z","iopub.execute_input":"2021-12-14T05:27:00.344009Z","iopub.status.idle":"2021-12-14T05:27:00.347668Z","shell.execute_reply.started":"2021-12-14T05:27:00.343977Z","shell.execute_reply":"2021-12-14T05:27:00.34689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(tf.keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self,ImageFeatures,tokenizer,maxLength,descriptions,vocabSize,batch_size=32):\n        'Initialization'\n        self.batch_size = batch_size\n        self.ImageFeatures = ImageFeatures\n        self.tokenizer = tokenizer\n        self.maxLength = maxLength\n        self.descriptions = descriptions\n        self.vocabSize = vocabSize\n        self.keys = list(descriptions.keys())\n        \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.descriptions) / self.batch_size))\n    \n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        batch_keys = self.keys[index*self.batch_size:(index+1)*self.batch_size]\n        batch_descriptions = { your_key: self.descriptions[your_key] for your_key in batch_keys }\n        batch_features = { your_key.split(\".\")[0]: self.ImageFeatures[your_key.split(\".\")[0]] for your_key in batch_keys}\n        X1,X2,Y = CreateSequence(self.tokenizer, self.maxLength, batch_descriptions,batch_features,self.vocabSize)\n        X_final = [X1, X2]\n        Y_final = Y\n        return X_final, Y_final\n                                 \n#     def CreateSequence(tokenizer, maxLength, desc, images, vocabSize):\n#         X1, X2, y = list(), list(), list()\n#         for key, desc_list in desc.items():\n#             #print(key)\n#             for desc_sent in desc_list:\n#                 #print(desc_sent)\n#                 sequence = tokenizer.texts_to_sequences([desc_sent])[0]\n#                 for i in range(1, len(sequence)):\n#                     in_sequence, out_sequence = sequence[:i], sequence[i]\n#                     in_sequence = pad_sequences([in_sequence], maxlen=maxLength)[0]\n#                     out_sequence = to_categorical([out_sequence], num_classes=vocabSize)[0]\n#                     X1.append(images[key[:-4]][0])\n#                     X2.append(in_sequence)\n#                     y.append(out_sequence)\n#                     # break\n#                 #break\n#             #break\n#         return array(X1), array(X2), array(y)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T05:27:04.695086Z","iopub.execute_input":"2021-12-14T05:27:04.695792Z","iopub.status.idle":"2021-12-14T05:27:04.70521Z","shell.execute_reply.started":"2021-12-14T05:27:04.695752Z","shell.execute_reply":"2021-12-14T05:27:04.704528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_generator = DataGenerator(train_features,tokenizer,max_length,train_descriptions,vocab_size)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T05:27:05.569517Z","iopub.execute_input":"2021-12-14T05:27:05.57006Z","iopub.status.idle":"2021-12-14T05:27:05.573809Z","shell.execute_reply.started":"2021-12-14T05:27:05.570024Z","shell.execute_reply":"2021-12-14T05:27:05.573141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = DefineModel(vocab_size, max_length)\nmodel.fit_generator(generator=training_generator,\n                    use_multiprocessing=True,\n                    epochs=40, verbose=2,\n                    workers=8)","metadata":{"execution":{"iopub.status.busy":"2021-12-14T05:27:06.457634Z","iopub.execute_input":"2021-12-14T05:27:06.458124Z","iopub.status.idle":"2021-12-14T07:38:21.959447Z","shell.execute_reply.started":"2021-12-14T05:27:06.458084Z","shell.execute_reply":"2021-12-14T07:38:21.95529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"indian_images_vgg_captions\")","metadata":{"execution":{"iopub.status.busy":"2021-12-14T07:39:04.988461Z","iopub.execute_input":"2021-12-14T07:39:04.988744Z","iopub.status.idle":"2021-12-14T07:39:30.558657Z","shell.execute_reply.started":"2021-12-14T07:39:04.98871Z","shell.execute_reply":"2021-12-14T07:39:30.557653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r ./final_image_vgg_captions.zip ./indian_images_vgg_captions","metadata":{"execution":{"iopub.status.busy":"2021-12-14T07:44:16.746833Z","iopub.execute_input":"2021-12-14T07:44:16.747153Z","iopub.status.idle":"2021-12-14T07:44:21.393344Z","shell.execute_reply.started":"2021-12-14T07:44:16.747117Z","shell.execute_reply":"2021-12-14T07:44:21.392524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}